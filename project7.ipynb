{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6830 Project 7\n",
    "Seth Beckett & Dave Storey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom Dataset\n",
    "This first part is the dataset that Seth worked with. It concerns classifying whether or not mushrooms are poisonous based on many different categorical attributes. More details about what the specific categories and decoding them can be found [here](https://www.kaggle.com/datasets/uciml/mushroom-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrooms = pd.read_csv(\"data/mushrooms.csv\")\n",
    "display(shrooms.columns)\n",
    "shrooms.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Models\n",
    "First we will try creating logistic regression models and a simple linear SVC with all of the data, and test how both perform. The GridSearchCV can take a while to fit, so I am just using the optimal parameters I found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shrooms.drop(\"class\", axis=1)\n",
    "X = pd.get_dummies(X)\n",
    "y = shrooms[\"class\"]\n",
    "y = y.map({\"p\": 1, \"e\": 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "print(\"Shape of data: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log_model = LogisticRegression(C=1, solver='liblinear', max_iter=1000)\n",
    "full_log_model.fit(X_train, y_train)\n",
    "y_pred = full_log_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_linear_svc = SVC(kernel='linear', C=1)\n",
    "full_linear_svc.fit(X_train, y_train)\n",
    "y_pred = full_linear_svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Features\n",
    "As we can see, both of these models end up with perfect accuracy, precision, and recall. We know that we have a lot of data, so in order to make our model simpler we will try using the top 5 features and see how that affects our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which features are affect logistic prediction the most\n",
    "log_coef = pd.DataFrame({\"feature\": X.columns, \"coef\": full_log_model.coef_[0]})\n",
    "log_coef['abs_coef'] = log_coef['coef'].abs()\n",
    "log_coef = log_coef.sort_values('abs_coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg performance with top 5 features\n",
    "top5_log_model = LogisticRegression(C=1, solver='liblinear', max_iter=1000)\n",
    "top5_log_model.fit(X_train[log_coef['feature'].iloc[:5]], y_train)\n",
    "y_pred = top5_log_model.predict(X_test[log_coef['feature'].iloc[:5]])\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=[\"edible\", \"poisonous\"])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for Top 5 Logistic Regression Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which features are affect SVC prediction the most\n",
    "svc_coef = pd.DataFrame({\"feature\": X.columns, \"coef\": full_linear_svc.coef_[0]})\n",
    "svc_coef['abs_coef'] = svc_coef['coef'].abs()\n",
    "svc_coef = svc_coef.sort_values('abs_coef', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC performance with top 5 features\n",
    "top5_svc = SVC(kernel='linear', C=1)\n",
    "top5_svc.fit(X_train[svc_coef['feature'].iloc[:5]], y_train)\n",
    "y_pred = top5_svc.predict(X_test[svc_coef['feature'].iloc[:5]])\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=[\"edible\", \"poisonous\"])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for Top 5 SVC Features\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Features\n",
    "That is insanely good for only considering 5 features. Let's see how accuracy is affected when we drop it down to the top 3 features for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate logreg performance with top 3 features\n",
    "top3_log_model = LogisticRegression(C=1, solver='liblinear', max_iter=1000)\n",
    "top3_log_model.fit(X_train[log_coef['feature'].iloc[:3]], y_train)\n",
    "y_pred = top3_log_model.predict(X_test[log_coef['feature'].iloc[:3]])\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=[\"edible\", \"poisonous\"])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for Top 3 Logistic Regression Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate SVC performance with top 3 features\n",
    "top3_svc = SVC(kernel='linear', C=1)\n",
    "top3_svc.fit(X_train[svc_coef['feature'].iloc[:3]], y_train)\n",
    "y_pred = top3_svc.predict(X_test[svc_coef['feature'].iloc[:3]])\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=[\"edible\", \"poisonous\"])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for Top 3 SVC Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top 5 logreg and top 5 svc features w/ their coefficients\n",
    "print(\"Top 5 Logistic Regression Features\")\n",
    "display(log_coef.iloc[:5])\n",
    "\n",
    "print(\"Top 5 SVC Features\")\n",
    "display(svc_coef.iloc[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see, the logistic regression model using the features \"odor_n\", \"spore-print-color_r\", and \"odor_l\" performs amazingly well. Both SVC and logistic regression only have 8 misclassifications when using their top 5 features, but SVC can't be trusted to save you from poisonous mushrooms if you only use the top 3 features.\n",
    "\n",
    "Decoding the odor variables goes as follows: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s. For spore print color, it goes as follows: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y.\n",
    "\n",
    "One interesting future direction one could take is to try looking at feature importance when not considering odor, since odor might be tricky to identify for some people."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Normalizes a dataframes integer columns\n",
    "def normalize(df):\n",
    "\tcopy = df.copy()\n",
    "\tfor col in df.columns:\n",
    "\t\tmax = df[col].max()\n",
    "\t\tmin = df[col].min()\n",
    "\t\tcopy[col] = (df[col]-min)/(max - min)\n",
    "\treturn copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('daveData/train.csv')\n",
    "df = df.dropna()\n",
    "df = df.drop(columns={'Unnamed: 0', 'id'})\n",
    "\n",
    "hotdf = pd.get_dummies(df)\n",
    "hotdf = normalize(hotdf)\n",
    "hotdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = hotdf.corr()\n",
    "best_features = corr_matrix.index\n",
    "plt.figure(figsize=(25,25))\n",
    "hot = sns.heatmap(hotdf[best_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hotdf['satisfaction_neutral or dissatisfied']\n",
    "x = hotdf.drop(columns={'satisfaction_neutral or dissatisfied', 'satisfaction_satisfied'})\n",
    "\n",
    "# The dataset is so big that I need to cut it down\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.7)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma=0.1)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Done Fitting the Model\")\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "print(f'Per: {p}')\n",
    "print(f'Rec: {r}')\n",
    "print(f'F1: {f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Note, this could take a while to run and is dependent upon the random splits above. However, Food and Drink is almost always in the top 10. This is the first thing that airlines can actually change/do proactivly. Inflight service is also a usual high feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_importance = permutation_importance(clf, x_test, y_test, n_jobs=-1, n_repeats=5)\n",
    "print(\"Done with Importance Stuff\")\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "for i in sorted_idx:\n",
    "    print(x.columns[i])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=2)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Done Fitting the Model\")\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "print(f'Per: {p}')\n",
    "print(f'Rec: {r}')\n",
    "print(f'F1: {f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "Increasing the class weight appears to increase the precision of our model. The recall suffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', class_weight={0:4})\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Done Fitting the Model\")\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "print(f'Per: {p}')\n",
    "print(f'Rec: {r}')\n",
    "print(f'F1: {f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LogisticRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "print(lm.score(x_train, y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotx = x_train[['Food and drink', 'Inflight service']]\n",
    "ploty = y_train\n",
    "color = ['r' if ploty_ == 0 else 'b' for ploty_ in y_train]\n",
    "plt.scatter(x_train['Food and drink'], x_train['Inflight service'], c=color)\n",
    "plt.xlabel(\"Food and Drink Rating\")\n",
    "plt.ylabel(\"Inflight Service Rating\")\n",
    "plt.savefig('FoodDrink.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
